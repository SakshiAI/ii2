{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEf1mVbyBOAn",
        "outputId": "ccaaa7cd-fc51-4f4d-e9a6-85e022c7d757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inverted Index:\n",
            "slept -> Document 2\n",
            "over -> Document 1\n",
            "lazy -> Document 1, Document 2\n",
            "quick -> Document 1\n",
            "the -> Document 1, Document 2\n",
            "in -> Document 2\n",
            "jumped -> Document 1\n",
            "dog -> Document 1, Document 2\n",
            "fox -> Document 1\n",
            "sun -> Document 2\n",
            ". -> Document 1, Document 2\n",
            "brown -> Document 1\n",
            "\n",
            "Enter your search query: lazy dog\n",
            "\n",
            "Documents matching the query:\n",
            "Document 1\n",
            "Document 2\n"
          ]
        }
      ],
      "source": [
        "# Import necessary modules\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download resources for nltk (only needed once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define the documents\n",
        "document1 = \"The quick brown fox jumped over the lazy dog .\"\n",
        "document2 = \"The lazy dog slept in the sun .\"\n",
        "\n",
        "# Step 1: Tokenize the documents\n",
        "# Convert each document to lowercase and split it into words\n",
        "tokens1 = word_tokenize(document1.lower())\n",
        "tokens2 = word_tokenize(document2.lower())\n",
        "#tokens1 = document1.lower().split()\n",
        "#tokens2 = document2.lower().split()\n",
        "\n",
        "# Remove only full stops (\".\") from tokens, but keep all other words intact\n",
        "#tokens1 = [token for token in tokens1 if token != \".\"]\n",
        "#tokens2 = [token for token in tokens2 if token != \".\"]\n",
        "\n",
        "# Combine the tokens into a list of unique terms\n",
        "terms = list(set(tokens1 + tokens2))\n",
        "\n",
        "# Step 2: Build the inverted index\n",
        "# Create an empty dictionary to store the inverted index\n",
        "inverted_index = {}\n",
        "\n",
        "# For each term, find the documents that contain it\n",
        "for term in terms:\n",
        "    documents = []\n",
        "    if term in tokens1:\n",
        "        documents.append(\"Document 1\")\n",
        "    if term in tokens2:\n",
        "        documents.append(\"Document 2\")\n",
        "    inverted_index[term] = documents\n",
        "\n",
        "# Step 3: Print the inverted index\n",
        "print(\"Inverted Index:\")\n",
        "for term, documents in inverted_index.items():\n",
        "    print(term, \"->\", \", \".join(documents))\n",
        "\n",
        "# Step 4: Search Query\n",
        "query = input(\"\\nEnter your search query: \").lower()  # Get the search query from the user\n",
        "query_terms = word_tokenize(query)  # Tokenize query into individual terms\n",
        "#query_terms = query.split()  (# Split query into individual terms)\n",
        "\n",
        "# Find the documents for the query\n",
        "result_docs = set()  # To store the matching documents\n",
        "\n",
        "# Iterate over the query terms and retrieve documents\n",
        "for term in query_terms:\n",
        "    if term in inverted_index:\n",
        "        result_docs.update(inverted_index[term])  # Add documents that contain the query term\n",
        "\n",
        "# Step 5: Display the results\n",
        "if result_docs:\n",
        "    print(\"\\nDocuments matching the query:\")\n",
        "    for doc in result_docs:\n",
        "        print(doc)\n",
        "else:\n",
        "    print(\"\\nNo documents found for the query.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yp3UZp-MBW5u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}